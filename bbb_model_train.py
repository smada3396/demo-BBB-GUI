# -*- coding: utf-8 -*-
"""bbb_model/train

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rqny2wr_srdvvZiNp7GxovNUCGvQBOLD
"""

import os
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from hyperopt import STATUS_OK, Trials, fmin, hp, tpe
from lightgbm import LGBMClassifier
from sklearn.metrics import (
    accuracy_score,
    average_precision_score,
    brier_score_loss,
    matthews_corrcoef,
    roc_auc_score,
    confusion_matrix,
)
from sklearn.model_selection import StratifiedKFold

from bbb_model_features_py import canonicalize_smiles, make_descriptor_matrix
from bbb_model_utils import ensure_dir, load_labeled_csv, save_joblib, save_json


@dataclass
class TrainPaths:
    bbb_internal: str
    efflux: str
    influx: str
    pampa: str
    cns: str
    bbbp_external: Optional[str] = None


def _canon_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["smiles_canon"] = df["smiles"].apply(canonicalize_smiles)
    df = df.dropna(subset=["smiles_canon"])
    df = df.drop_duplicates(subset=["smiles_canon"])
    return df


def _build_Xy(df: pd.DataFrame, descriptor_cols: Optional[List[str]] = None) -> Tuple[pd.DataFrame, np.ndarray]:
    smiles = df["smiles_canon"].tolist()
    X = make_descriptor_matrix(smiles, do_variance_filter=False, do_corr_prune=False)
    if descriptor_cols is not None:
        # align columns
        missing = [c for c in descriptor_cols if c not in X.columns]
        for c in missing:
            X[c] = 0.0
        X = X[descriptor_cols]
    y = df["label"].values.astype(int)
    return X, y


def _fit_lgbm(X: pd.DataFrame, y: np.ndarray, params: Dict, seed: int) -> LGBMClassifier:
    model = LGBMClassifier(
        random_state=seed,
        n_estimators=int(params["n_estimators"]),
        learning_rate=float(params["learning_rate"]),
        max_depth=int(params["max_depth"]),
        num_leaves=int(params["num_leaves"]),
        min_child_samples=int(params["min_child_samples"]),
        subsample=float(params["subsample"]),
        colsample_bytree=float(params["colsample_bytree"]),
        reg_alpha=float(params["reg_alpha"]),
        reg_lambda=float(params["reg_lambda"]),
        class_weight=params.get("class_weight", None),
    )
    model.fit(X, y)
    return model


def _hyperopt_space() -> Dict:
    return {
        "n_estimators": hp.quniform("n_estimators", 200, 2000, 50),
        "learning_rate": hp.loguniform("learning_rate", np.log(0.01), np.log(0.2)),
        "max_depth": hp.quniform("max_depth", 3, 10, 1),
        "num_leaves": hp.quniform("num_leaves", 16, 256, 8),
        "min_child_samples": hp.quniform("min_child_samples", 10, 200, 5),
        "subsample": hp.uniform("subsample", 0.6, 1.0),
        "colsample_bytree": hp.uniform("colsample_bytree", 0.6, 1.0),
        "reg_alpha": hp.loguniform("reg_alpha", np.log(1e-8), np.log(10.0)),
        "reg_lambda": hp.loguniform("reg_lambda", np.log(1e-8), np.log(10.0)),
    }


def _cv_auc_objective(
    X: pd.DataFrame,
    y: np.ndarray,
    seed: int,
    n_splits: int,
):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)

    def objective(params):
        # cast
        params = params.copy()
        for k in ["n_estimators", "max_depth", "num_leaves", "min_child_samples"]:
            params[k] = int(params[k])

        aucs = []
        for tr_idx, va_idx in skf.split(X, y):
            Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]
            ytr, yva = y[tr_idx], y[va_idx]

            # class weights if imbalanced
            pos = max(1, int((ytr == 1).sum()))
            neg = max(1, int((ytr == 0).sum()))
            w_pos = neg / pos
            class_weight = {0: 1.0, 1: float(w_pos)}
            params["class_weight"] = class_weight

            model = _fit_lgbm(Xtr, ytr, params, seed)
            p = model.predict_proba(Xva)[:, 1]
            aucs.append(roc_auc_score(yva, p))

        return {"loss": -float(np.mean(aucs)), "status": STATUS_OK}

    return objective


def tune_lgbm(
    X: pd.DataFrame,
    y: np.ndarray,
    *,
    seed: int = 42,
    n_splits: int = 5,
    n_trials: int = 100,
) -> Dict:
    space = _hyperopt_space()
    trials = Trials()
    best = fmin(
        fn=_cv_auc_objective(X, y, seed, n_splits),
        space=space,
        algo=tpe.suggest,
        max_evals=n_trials,
        trials=trials,
        rstate=np.random.default_rng(seed),
    )
    # cast back
    best["n_estimators"] = int(best["n_estimators"])
    best["max_depth"] = int(best["max_depth"])
    best["num_leaves"] = int(best["num_leaves"])
    best["min_child_samples"] = int(best["min_child_samples"])
    return best


def _metrics(y_true: np.ndarray, p: np.ndarray, threshold: float = 0.5) -> Dict:
    y_hat = (p >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()
    sens = tp / (tp + fn) if (tp + fn) else 0.0
    spec = tn / (tn + fp) if (tn + fp) else 0.0
    bal_acc = 0.5 * (sens + spec)
    return {
        "auroc": float(roc_auc_score(y_true, p)),
        "auprc": float(average_precision_score(y_true, p)),
        "accuracy": float(accuracy_score(y_true, y_hat)),
        "brier": float(brier_score_loss(y_true, p)),
        "mcc": float(matthews_corrcoef(y_true, y_hat)),
        "sensitivity": float(sens),
        "specificity": float(spec),
        "balanced_accuracy": float(bal_acc),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn),
        "tp": int(tp),
    }


def train_two_stage(
    paths: TrainPaths,
    *,
    artifacts_dir: str = "artifacts",
    seed: int = 42,
    n_splits: int = 5,
    n_trials: int = 100,
) -> None:
    ensure_dir(artifacts_dir)

    # Load and canonicalize
    bbb = _canon_df(load_labeled_csv(paths.bbb_internal))
    efflux = _canon_df(load_labeled_csv(paths.efflux))
    influx = _canon_df(load_labeled_csv(paths.influx))
    pampa = _canon_df(load_labeled_csv(paths.pampa))
    cns = _canon_df(load_labeled_csv(paths.cns))

    # Descriptor matrix for BBB set, then filter columns (variance + correlation)
    X_raw = make_descriptor_matrix(bbb["smiles_canon"].tolist(), do_variance_filter=True, do_corr_prune=True, corr_threshold=0.95)
    descriptor_cols = X_raw.columns.tolist()
    save_json(descriptor_cols, os.path.join(artifacts_dir, "descriptor_cols.json"))

    # Rebuild BBB X with frozen descriptor columns
    X_bbb, y_bbb = _build_Xy(bbb, descriptor_cols=descriptor_cols)

    # Helper: build X,y for mechanism tasks using same descriptor columns
    X_eff, y_eff = _build_Xy(efflux, descriptor_cols=descriptor_cols)
    X_inf, y_inf = _build_Xy(influx, descriptor_cols=descriptor_cols)
    X_pam, y_pam = _build_Xy(pampa, descriptor_cols=descriptor_cols)
    X_cns, y_cns = _build_Xy(cns, descriptor_cols=descriptor_cols)

    # Tune Stage 1 models
    print("Tuning Stage 1 models...")
    best_eff = tune_lgbm(X_eff, y_eff, seed=seed, n_splits=n_splits, n_trials=n_trials)
    best_inf = tune_lgbm(X_inf, y_inf, seed=seed, n_splits=n_splits, n_trials=n_trials)
    best_pam = tune_lgbm(X_pam, y_pam, seed=seed, n_splits=n_splits, n_trials=n_trials)
    best_cns = tune_lgbm(X_cns, y_cns, seed=seed, n_splits=n_splits, n_trials=n_trials)

    save_json(best_eff, os.path.join(artifacts_dir, "params_stage1_efflux.json"))
    save_json(best_inf, os.path.join(artifacts_dir, "params_stage1_influx.json"))
    save_json(best_pam, os.path.join(artifacts_dir, "params_stage1_pampa.json"))
    save_json(best_cns, os.path.join(artifacts_dir, "params_stage1_cns.json"))

    # Build out of fold mechanistic probabilities for BBB (stacking features)
    print("Generating out of fold mechanistic probabilities for BBB set...")
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)

    p_eff_oof = np.zeros(len(bbb), dtype=float)
    p_inf_oof = np.zeros(len(bbb), dtype=float)
    p_pam_oof = np.zeros(len(bbb), dtype=float)
    p_cns_oof = np.zeros(len(bbb), dtype=float)

    # To reduce leakage if there is overlap between BBB compounds and mechanism datasets,
    # exclude held out fold compounds (by canonical smiles) from mechanism training in each fold.
    bbb_smiles = bbb["smiles_canon"].tolist()

    efflux_map = dict(zip(efflux["smiles_canon"], efflux["label"]))
    influx_map = dict(zip(influx["smiles_canon"], influx["label"]))
    pampa_map = dict(zip(pampa["smiles_canon"], pampa["label"]))
    cns_map = dict(zip(cns["smiles_canon"], cns["label"]))

    efflux_canon = efflux["smiles_canon"].tolist()
    influx_canon = influx["smiles_canon"].tolist()
    pampa_canon = pampa["smiles_canon"].tolist()
    cns_canon = cns["smiles_canon"].tolist()

    # Precompute mechanism descriptor matrices once
    X_eff_full, y_eff_full = X_eff, y_eff
    X_inf_full, y_inf_full = X_inf, y_inf
    X_pam_full, y_pam_full = X_pam, y_pam
    X_cns_full, y_cns_full = X_cns, y_cns

    for tr_idx, va_idx in skf.split(X_bbb, y_bbb):
        heldout_smiles = set([bbb_smiles[i] for i in va_idx])

        # filter each mechanism dataset to exclude heldout smiles if present there
        def filter_mech(Xm, ym, canon_list):
            keep_idx = [i for i, s in enumerate(canon_list) if s not in heldout_smiles]
            if len(keep_idx) < 50:
                # fallback: if filtering makes it too small, do not filter
                return Xm, ym
            return Xm.iloc[keep_idx], ym[keep_idx]

        Xeff_tr, yeff_tr = filter_mech(X_eff_full, y_eff_full, efflux_canon)
        Xinf_tr, yinf_tr = filter_mech(X_inf_full, y_inf_full, influx_canon)
        Xpam_tr, ypam_tr = filter_mech(X_pam_full, y_pam_full, pampa_canon)
        Xcns_tr, ycns_tr = filter_mech(X_cns_full, y_cns_full, cns_canon)

        # train stage 1 models for this fold
        meff = _fit_lgbm(Xeff_tr, yeff_tr, {**best_eff}, seed)
        minf = _fit_lgbm(Xinf_tr, yinf_tr, {**best_inf}, seed)
        mpam = _fit_lgbm(Xpam_tr, ypam_tr, {**best_pam}, seed)
        mcns = _fit_lgbm(Xcns_tr, ycns_tr, {**best_cns}, seed)

        Xva = X_bbb.iloc[va_idx]
        p_eff_oof[va_idx] = meff.predict_proba(Xva)[:, 1]
        p_inf_oof[va_idx] = minf.predict_proba(Xva)[:, 1]
        p_pam_oof[va_idx] = mpam.predict_proba(Xva)[:, 1]
        p_cns_oof[va_idx] = mcns.predict_proba(Xva)[:, 1]

    X_stage2 = X_bbb.copy()
    X_stage2["p_efflux_mech"] = p_eff_oof
    X_stage2["p_influx_mech"] = p_inf_oof
    X_stage2["p_pampa_mech"] = p_pam_oof
    X_stage2["p_cns_mech"] = p_cns_oof

    stage2_cols = X_stage2.columns.tolist()
    save_json(stage2_cols, os.path.join(artifacts_dir, "stage2_feature_cols.json"))

    # Tune Stage 2
    print("Tuning Stage 2 model...")
    best_stage2 = tune_lgbm(X_stage2, y_bbb, seed=seed, n_splits=n_splits, n_trials=n_trials)
    save_json(best_stage2, os.path.join(artifacts_dir, "params_stage2.json"))

    # Cross validated metrics for Stage 2
    print("Computing cross validated metrics for Stage 2...")
    cv_metrics = []
    for tr_idx, va_idx in skf.split(X_stage2, y_bbb):
        Xtr, Xva = X_stage2.iloc[tr_idx], X_stage2.iloc[va_idx]
        ytr, yva = y_bbb[tr_idx], y_bbb[va_idx]

        pos = max(1, int((ytr == 1).sum()))
        neg = max(1, int((ytr == 0).sum()))
        w_pos = neg / pos
        params = {**best_stage2, "class_weight": {0: 1.0, 1: float(w_pos)}}
        m2 = _fit_lgbm(Xtr, ytr, params, seed)
        p = m2.predict_proba(Xva)[:, 1]
        cv_metrics.append(_metrics(yva, p, threshold=0.5))

    save_json(cv_metrics, os.path.join(artifacts_dir, "cv_metrics_stage2.json"))

    # Fit final Stage 1 models on all mechanism datasets
    print("Fitting final Stage 1 models on full mechanism datasets...")
    meff_final = _fit_lgbm(X_eff_full, y_eff_full, {**best_eff}, seed)
    minf_final = _fit_lgbm(X_inf_full, y_inf_full, {**best_inf}, seed)
    mpam_final = _fit_lgbm(X_pam_full, y_pam_full, {**best_pam}, seed)
    mcns_final = _fit_lgbm(X_cns_full, y_cns_full, {**best_cns}, seed)

    # Build final Stage 2 training features using final Stage 1 predictions on BBB set
    p_eff_full = meff_final.predict_proba(X_bbb)[:, 1]
    p_inf_full = minf_final.predict_proba(X_bbb)[:, 1]
    p_pam_full = mpam_final.predict_proba(X_bbb)[:, 1]
    p_cns_full = mcns_final.predict_proba(X_bbb)[:, 1]

    X_stage2_full = X_bbb.copy()
    X_stage2_full["p_efflux_mech"] = p_eff_full
    X_stage2_full["p_influx_mech"] = p_inf_full
    X_stage2_full["p_pampa_mech"] = p_pam_full
    X_stage2_full["p_cns_mech"] = p_cns_full
    X_stage2_full = X_stage2_full[stage2_cols]

    print("Fitting final Stage 2 model...")
    pos = max(1, int((y_bbb == 1).sum()))
    neg = max(1, int((y_bbb == 0).sum()))
    w_pos = neg / pos
    params2 = {**best_stage2, "class_weight": {0: 1.0, 1: float(w_pos)}}
    m2_final = _fit_lgbm(X_stage2_full, y_bbb, params2, seed)

    # Save artifacts
    ensure_dir(os.path.join(artifacts_dir, "models"))
    save_joblib(meff_final, os.path.join(artifacts_dir, "models", "stage1_efflux.joblib"))
    save_joblib(minf_final, os.path.join(artifacts_dir, "models", "stage1_influx.joblib"))
    save_joblib(mpam_final, os.path.join(artifacts_dir, "models", "stage1_pampa.joblib"))
    save_joblib(mcns_final, os.path.join(artifacts_dir, "models", "stage1_cns.joblib"))
    save_joblib(m2_final, os.path.join(artifacts_dir, "models", "stage2_bbb.joblib"))

    # Optional external evaluation if BBBP has labels
    if paths.bbbp_external is not None and os.path.exists(paths.bbbp_external):
        bbbp = _canon_df(load_labeled_csv(paths.bbbp_external))
        X_bbbp, y_bbbp = _build_Xy(bbbp, descriptor_cols=descriptor_cols)

        p_eff = meff_final.predict_proba(X_bbbp)[:, 1]
        p_inf = minf_final.predict_proba(X_bbbp)[:, 1]
        p_pam = mpam_final.predict_proba(X_bbbp)[:, 1]
        p_cns = mcns_final.predict_proba(X_bbbp)[:, 1]

        X_bbbp2 = X_bbbp.copy()
        X_bbbp2["p_efflux_mech"] = p_eff
        X_bbbp2["p_influx_mech"] = p_inf
        X_bbbp2["p_pampa_mech"] = p_pam
        X_bbbp2["p_cns_mech"] = p_cns
        X_bbbp2 = X_bbbp2[stage2_cols]

        p_final = m2_final.predict_proba(X_bbbp2)[:, 1]
        ext_metrics = _metrics(y_bbbp, p_final, threshold=0.5)
        save_json(ext_metrics, os.path.join(artifacts_dir, "external_metrics_bbbp.json"))

    print("Done. Artifacts saved to:", artifacts_dir)


if __name__ == "__main__":
    # Edit paths here or pass through your own wrapper
    paths = TrainPaths(
        bbb_internal="data/bbb_internal.csv",
        efflux="data/efflux.csv",
        influx="data/influx.csv",
        pampa="data/pampa.csv",
        cns="data/cns.csv",
        bbbp_external="data/bbbp_external.csv",
    )
    train_two_stage(paths, artifacts_dir="artifacts", seed=42, n_splits=5, n_trials=100)
